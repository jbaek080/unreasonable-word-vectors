{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "loc = './GoogleNews-vectors-negative300.bin'\n",
    "w2vmodel = gensim.models.KeyedVectors.load_word2vec_format(loc, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t-tokuku\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824870109558105),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('teenage_girl', 0.6147903800010681),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('suspected_purse_snatcher', 0.5716364979743958),\n",
       " ('robber', 0.5585119128227234),\n",
       " ('Robbery_suspect', 0.5584409236907959),\n",
       " ('teen_ager', 0.5549196600914001),\n",
       " ('men', 0.5489763021469116),\n",
       " ('horribly_horribly_deranged', 0.5426712036132812),\n",
       " ('guy', 0.5420035123825073),\n",
       " ('person', 0.5342026352882385),\n",
       " ('gentleman', 0.5337990522384644),\n",
       " ('knife_wielding_thief', 0.5337865352630615),\n",
       " ('motorcyclist', 0.5336882472038269),\n",
       " ('Gunshot_victim', 0.5318589210510254),\n",
       " ('Man', 0.5316052436828613),\n",
       " ('Stabbing_suspect', 0.5304680466651917),\n",
       " ('Escaped_prisoner', 0.5298845171928406)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.most_similar('man', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t-tokuku\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('man', 0.7664012908935547),\n",
       " ('girl', 0.7494640946388245),\n",
       " ('teenage_girl', 0.7336829900741577),\n",
       " ('teenager', 0.631708562374115),\n",
       " ('lady', 0.6288785934448242),\n",
       " ('teenaged_girl', 0.6141783595085144),\n",
       " ('mother', 0.607630729675293),\n",
       " ('policewoman', 0.6069462299346924),\n",
       " ('boy', 0.5975908041000366),\n",
       " ('Woman', 0.5770983695983887),\n",
       " ('sexually_assualted', 0.5723767876625061),\n",
       " ('she', 0.5641393661499023),\n",
       " ('Leah_Questin', 0.5481955409049988),\n",
       " ('WOMAN', 0.5480420589447021),\n",
       " ('person', 0.5470173358917236),\n",
       " ('housewife', 0.5463823080062866),\n",
       " ('victim', 0.545007586479187),\n",
       " ('daughter', 0.5442972779273987),\n",
       " ('grandmother', 0.5435301661491394),\n",
       " ('schoolgirl', 0.5430024862289429)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.most_similar('woman', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t-tokuku\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wife', 0.8294166326522827),\n",
       " ('mother', 0.7414050102233887),\n",
       " ('father', 0.7408472299575806),\n",
       " ('daughter', 0.7383991479873657),\n",
       " ('fianc√©', 0.7353568077087402),\n",
       " ('fiance', 0.7343953847885132),\n",
       " ('son', 0.7240972518920898),\n",
       " ('boyfriend', 0.6959607005119324),\n",
       " ('hus_band', 0.6928317546844482),\n",
       " ('stepfather', 0.6896491646766663)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.most_similar('husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2vmodel['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\t-tokuku\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\t-tokuku\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.1291504e-03, -8.9645386e-04,  3.1852722e-04, ...,\n",
       "        -1.5640259e-03, -1.2302399e-04, -8.6307526e-05],\n",
       "       [ 7.0312500e-02,  8.6914062e-02,  8.7890625e-02, ...,\n",
       "        -4.7607422e-02,  1.4465332e-02, -6.2500000e-02],\n",
       "       [-1.1779785e-02, -4.7363281e-02,  4.4677734e-02, ...,\n",
       "         7.1289062e-02, -3.4912109e-02,  2.4169922e-02],\n",
       "       ...,\n",
       "       [-1.9653320e-02, -9.0820312e-02, -1.9409180e-02, ...,\n",
       "        -1.6357422e-02, -1.3427734e-02,  4.6630859e-02],\n",
       "       [ 3.2714844e-02, -3.2226562e-02,  3.6132812e-02, ...,\n",
       "        -8.8500977e-03,  2.6977539e-02,  1.9042969e-02],\n",
       "       [ 4.5166016e-02, -4.5166016e-02, -3.9367676e-03, ...,\n",
       "         7.9589844e-02,  7.2265625e-02,  1.3000488e-02]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Vocabulary from the sentences\n",
      "Vocabulary size = 49815\n",
      "Total words to be trained = 1161192\n",
      "Deleting the words which occur less than 2 times\n",
      "Vocabulary size after filtering words = 27806\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Lambda, Dense, merge, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.objectives import mse\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from sentences_generator import Sentences, GutenbergSentences, BrownSentences\n",
    "import vocab_generator as V_gen\n",
    "import save_embeddings as S\n",
    "import global_settings as G\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sentences = BrownSentences()\n",
    "vocabulary = dict()\n",
    "V_gen.build_vocabulary(vocabulary, sentences)\n",
    "V_gen.filter_vocabulary_based_on(vocabulary, G.min_count)\n",
    "reverse_vocabulary, non_reverse_vocabulary = V_gen.generate_vocabulary_lookups(vocabulary, \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = [0 for _ in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = [unk, unk]\n",
    "for i in range(2, len(non_reverse_vocabulary) + 2):\n",
    "    word = non_reverse_vocabulary[i]\n",
    "    if word in w2vmodel:\n",
    "        embedding.append(w2vmodel[word])\n",
    "    else:\n",
    "        r = np.random.uniform(-1.0/2.0/g_embed_dim, 1.0/2.0/g_embed_dim, (g_embed_dim,))\n",
    "        embedding.append(r)\n",
    "        \n",
    "embedding.append(np.random.uniform(-1.0/2.0/g_embed_dim, 1.0/2.0/g_embed_dim, (g_embed_dim,)))\n",
    "embedding.append(np.random.uniform(-1.0/2.0/g_embed_dim, 1.0/2.0/g_embed_dim, (g_embed_dim,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.array(embedding, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27809, 300)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_52 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_53 (InputLayer)           (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        multiple             8342700     input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_54 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 300)          0           embedding_18[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot_12 (Dot)                    (None, 1)            0           embedding_18[0][0]               \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_13 (Dot)                    (None, 5)            0           embedding_18[2][0]               \n",
      "                                                                 lambda_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,342,700\n",
      "Trainable params: 8,342,700\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "   76/27806 [..............................] - ETA: 47:31 - loss: 0.1557 - dot_12_loss: 0.4568 - dot_13_loss: -0.3011"
     ]
    }
   ],
   "source": [
    "def cos_similarity(a, b):\n",
    "    return np.abs(-cosine(a, b) + 1)\n",
    "\n",
    "def similarity(w1, w2, word_embeddings):\n",
    "    return cos_similarity(word_embeddings[reverse_vocabulary[w1]], word_embeddings[reverse_vocabulary[w2]])\n",
    "\n",
    "def n_most_similar(word_inp, word_embeddings, n=5):\n",
    "    word_to_similarity = {}\n",
    "    for i in range(2, len(word_embeddings) - 2):\n",
    "        word = non_reverse_vocabulary[i]\n",
    "        word_vector = word_embeddings[i]\n",
    "        sim = similarity(word_inp, word, word_embeddings)\n",
    "        word_to_similarity[word] = sim\n",
    "\n",
    "    top_n = sorted(word_to_similarity.items(), key=lambda x: -x[1])[:n]\n",
    "    return top_n\n",
    "\n",
    "def analogy(a, b, c, word_embeddings, n = 5):\n",
    "    word_to_similarity = {}\n",
    "    import pdb; pdb.set_trace()\n",
    "    a = word_embeddings[reverse_vocabulary[a]]\n",
    "    b = word_embeddings[reverse_vocabulary[b]]\n",
    "    c = word_embeddings[reverse_vocabulary[c]]\n",
    "    for i in range(2, len(word_embeddings) - 2):\n",
    "        word = non_reverse_vocabulary[i]\n",
    "        d = word_embeddings[i]\n",
    "        sim = cos_similarity(a - b, c - d)\n",
    "        word_to_similarity[word] = sim\n",
    "    top_n = sorted(word_to_similarity.items(), key=lambda x: -x[1])[:n]\n",
    "    return top_n\n",
    "    \n",
    "\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    embeddings = []\n",
    "    with open(filename) as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            word_and_vec = embeddings.split(\"\\t\")\n",
    "            word = word_and_vec[0]\n",
    "            vec = [float(i) for i in word_and_vec[1].split(\" \")]\n",
    "            embeddings.append(vec)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "k = G.window_size # context windows size\n",
    "context_size = 2*k\n",
    "\n",
    "# Creating CBOW model\n",
    "# Model has 3 inputs\n",
    "# Current word index, context words indexes and negative sampled word indexes\n",
    "word_index = Input(shape=(1,))\n",
    "context = Input(shape=(context_size,))\n",
    "negative_samples = Input(shape=(G.negative,))\n",
    "\n",
    "g_embed_dim = 300\n",
    "# All the inputs are processed through a common embedding layer\n",
    "shared_embedding_layer = Embedding(input_dim=(G.vocab_size+3), output_dim=g_embed_dim, weights=[embedding])\n",
    "word_embedding = shared_embedding_layer(word_index)\n",
    "context_embeddings = shared_embedding_layer(context)\n",
    "negative_words_embedding = shared_embedding_layer(negative_samples)\n",
    "# Now the context words are averaged to get the CBOW vector\n",
    "cbow = Lambda(lambda x: K.mean(x, axis=1), output_shape=(g_embed_dim,))(context_embeddings)\n",
    "# The context is multiplied (dot product) with current word and negative sampled words\n",
    "word_context_product = dot([word_embedding, cbow], axes=-1)\n",
    "negative_context_product = dot([negative_words_embedding, cbow], axes=-1)\n",
    "# The dot products are outputted\n",
    "model = Model(inputs=[word_index, context, negative_samples], outputs=[word_context_product, negative_context_product])\n",
    "i_woman = reverse_vocabulary['woman']\n",
    "i_man = reverse_vocabulary['man']\n",
    "\n",
    "def loss_mse(y_true, y_pred, alpha = 1):\n",
    "    word_embeddings = shared_embedding_layer.get_weights()[0]\n",
    "    woman = word_embeddings[i_woman]\n",
    "    man = word_embeddings[i_man]\n",
    "    return binary_crossentropy(y_true, y_pred) - alpha * cos_similarity(woman, man)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=loss_mse)\n",
    "print(model.summary())\n",
    "\n",
    "word_embeddings = shared_embedding_layer.get_weights()[0]\n",
    "\n",
    "gen = V_gen.pretraining_batch_generator(sentences, vocabulary, reverse_vocabulary) \n",
    "model.fit_generator(gen, steps_per_epoch=len(vocabulary), epochs=1)\n",
    "# Save the trained embedding\n",
    "pickle.dump(shared_embedding_layer.get_weights()[0], open(\"embeddings.pkl\", \"wb\"))\n",
    "S.save_embeddings(\"embedding.txt\", shared_embedding_layer.get_weights()[0], vocabulary)\n",
    "\n",
    "word_embeddings = shared_embedding_layer.get_weights()[0]\n",
    "print(\"similarity between woman and man: \", str(cos_similarity(word_embeddings[i_woman], word_embeddings[i_man])))\n",
    "print(\"similarity between husband and wife: \", str(cos_similarity(word_embeddings[reverse_vocabulary['husband']], word_embeddings[reverse_vocabulary['wife']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
